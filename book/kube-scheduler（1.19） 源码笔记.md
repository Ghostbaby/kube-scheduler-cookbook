# kube-schedulerï¼ˆ1.19ï¼‰ æºç ç¬”è®°

## SchedulingQueueä¸‰çº§è°ƒåº¦é˜Ÿåˆ—

![kube-scheduler-SchedulingQueue](../image/kube-scheduler-SchedulingQueue.png)

SchedulingQueue æ˜¯ä¸€ä¸ª Interfaceï¼Œ ä¸»è¦æä¾›ç”¨ä»¥å®ç°å¯¹ Pod çš„å…¥é˜Ÿå‡ºé˜Ÿæ“ä½œï¼Œä¸Šå›¾ä¸­ `PodBackoffMap` å·²ç»è¢«æ›¿æ¢æˆ`QueuedPodInfo`ï¼Œ`QueuedPodInfo`å°†è´¯ç©¿äºæ•´ä¸ªä¸‰ä¸ªè°ƒåº¦é˜Ÿåˆ—ä¸­ã€‚

```go
// QueuedPodInfo is a Pod wrapper with additional information related to
// the pod's status in the scheduling queue, such as the timestamp when
// it's added to the queue.
type QueuedPodInfo struct {
   Pod *v1.Pod
   // The time pod added to the scheduling queue.
  // ä¸Šæ¬¡è¢«è°ƒåº¦çš„æ—¶é—´æˆ³
   Timestamp time.Time
   // Number of schedule attempts before successfully scheduled.
   // It's used to record the # attempts metric.
  // è°ƒåº¦æˆåŠŸä¹‹å‰é‡è¯•æ¬¡æ•°
   Attempts int
   // The time when the pod is added to the queue for the first time. The pod may be added
   // back to the queue multiple times before it's successfully scheduled.
   // It shouldn't be updated once initialized. It's used to record the e2e scheduling
   // latency for a pod.
  // podç¬¬ä¸€æ¬¡è¢«è°ƒåº¦çš„æ—¶é—´ï¼Œåœ¨è¢«åˆå§‹åŒ–ä¹‹åï¼Œè¯¥æ—¶é—´ä¸ä¼šè¢«æ›´æ–°ï¼Œä¸»è¦ç”¨äºè®¡ç®—è°ƒåº¦è€—æ—¶
   InitialAttemptTimestamp time.Time
}
```



```go
// SchedulingQueue is an interface for a queue to store pods waiting to be scheduled.
// The interface follows a pattern similar to cache.FIFO and cache.Heap and
// makes it easy to use those data structures as a SchedulingQueue.
type SchedulingQueue interface {
   framework.PodNominator
   Add(pod *v1.Pod) error
   // AddUnschedulableIfNotPresent adds an unschedulable pod back to scheduling queue.
   // The podSchedulingCycle represents the current scheduling cycle number which can be
   // returned by calling SchedulingCycle().
   AddUnschedulableIfNotPresent(pod *framework.QueuedPodInfo, podSchedulingCycle int64) error
   // SchedulingCycle returns the current number of scheduling cycle which is
   // cached by scheduling queue. Normally, incrementing this number whenever
   // a pod is popped (e.g. called Pop()) is enough.
   SchedulingCycle() int64
   // Pop removes the head of the queue and returns it. It blocks if the
   // queue is empty and waits until a new item is added to the queue.
   Pop() (*framework.QueuedPodInfo, error)
   Update(oldPod, newPod *v1.Pod) error
   Delete(pod *v1.Pod) error
   MoveAllToActiveOrBackoffQueue(event string)
   AssignedPodAdded(pod *v1.Pod)
   AssignedPodUpdated(pod *v1.Pod)
   PendingPods() []*v1.Pod
   // Close closes the SchedulingQueue so that the goroutine which is
   // waiting to pop items can exit gracefully.
   Close()
   // NumUnschedulablePods returns the number of unschedulable pods exist in the SchedulingQueue.
   NumUnschedulablePods() int
   // Run starts the goroutines managing the queue.
   Run()
}
```

å®é™…ä¸»è¦é€šè¿‡ `PriorityQueue` æ¥å®ç°è°ƒåº¦é˜Ÿåˆ—

```go
// PriorityQueue implements a scheduling queue.
// The head of PriorityQueue is the highest priority pending pod. This structure
// has three sub queues. One sub-queue holds pods that are being considered for
// scheduling. This is called activeQ and is a Heap. Another queue holds
// pods that are already tried and are determined to be unschedulable. The latter
// is called unschedulableQ. The third queue holds pods that are moved from
// unschedulable queues and will be moved to active queue when backoff are completed.
type PriorityQueue struct {
   // PodNominator abstracts the operations to maintain nominated Pods.
   framework.PodNominator

   stop  chan struct{}
   clock util.Clock

   // pod initial backoff duration.
   podInitialBackoffDuration time.Duration
   // pod maximum backoff duration.
   podMaxBackoffDuration time.Duration

   lock sync.RWMutex
   cond sync.Cond

   // activeQ is heap structure that scheduler actively looks at to find pods to
   // schedule. Head of heap is the highest priority pod.
   activeQ *heap.Heap
   // podBackoffQ is a heap ordered by backoff expiry. Pods which have completed backoff
   // are popped from this heap before the scheduler looks at activeQ
   podBackoffQ *heap.Heap
   // unschedulableQ holds pods that have been tried and determined unschedulable.
   unschedulableQ *UnschedulablePodsMap
   // schedulingCycle represents sequence number of scheduling cycle and is incremented
   // when a pod is popped.
   schedulingCycle int64
   // moveRequestCycle caches the sequence number of scheduling cycle when we
   // received a move request. Unscheduable pods in and before this scheduling
   // cycle will be put back to activeQueue if we were trying to schedule them
   // when we received move request.
   moveRequestCycle int64

   // closed indicates that the queue is closed.
   // It is mainly used to let Pop() exit its control loop while waiting for an item.
   closed bool
}
```

å…¶ä¸­åŒ…å«å¦‚ä¸‹ä¸‰çº§è°ƒåº¦é˜Ÿåˆ—ï¼š

- activeQï¼Œæ´»åŠ¨é˜Ÿåˆ—ï¼Œä¸»è¦ç”¨ä»¥å­˜å‚¨å½“å‰æ‰€æœ‰æ­£åœ¨ç­‰å¾…è°ƒåº¦çš„Pod
- unschedulableQï¼Œ ä¸å¯è°ƒåº¦é˜Ÿåˆ—ï¼Œå½“Podç”³è¯·çš„èµ„æºåœ¨å½“å‰é›†ç¾¤ä¸­æ— æ³•å¾—åˆ°æ»¡è¶³æ—¶ï¼Œå°†ä¼šè¢«è°ƒåº¦è‡³è¯¥é˜Ÿåˆ—ä¸­ï¼Œå½“é›†ç¾¤èµ„æºå‘ç”Ÿå˜åŒ–æ—¶ï¼Œå†æ¬¡å¯¹è¯¥é˜Ÿåˆ—è¿›è¡Œè°ƒåº¦å°è¯•
- podBackoffQï¼Œå¤±è´¥é˜Ÿåˆ—ï¼Œå½“podè°ƒåº¦å¤±è´¥ä¹‹åå°†ä¼šå¢åŠ åˆ°è¯¥é˜Ÿåˆ—ï¼Œç­‰å¾…åç»­é‡è¯•ï¼Œåå¤è°ƒåº¦å¤±è´¥çš„Podå°†ä¼šæŒ‰æ­¤å¢é•¿ç­‰å¾…æ—¶é—´ï¼Œé™ä½é‡è¯•æ•ˆç‡ã€‚

podBackoffQ å’Œ unschedulableQï¼Œä¼šå®šæ—¶ä»å‰é¢ä¸¤ä¸ªé˜Ÿåˆ—ä¸­æ‹¿å‡ºPodæ”¾åˆ°activeQé˜Ÿåˆ—ã€‚

- æ¯éš”1ç§’æ‰§è¡Œ `flushBackoffQCompleted`ï¼Œå»æ‰¾åˆ°backoffQä¸­ç­‰å¾…åˆ°æœŸçš„Podï¼Œå°†å…¶æ”¾å…¥åˆ°activeQä¸­
- æ¯éš”30ç§’æ‰§è¡Œ `flushUnschedulableQLeftover`ï¼Œå¦‚æœå½“å‰æ—¶é—´-podçš„æœ€åè°ƒåº¦æ—¶é—´å¤§äº60s,å°±é‡æ–°è°ƒåº¦ï¼Œè½¬ç§»åˆ°podBackoffQæˆ–è€…activeQä¸­

### ActiveQ 

#### èµ„æºå‘ç”Ÿå˜åŒ–æ—¶

å½“æœ‰æ–° Pod è¢«åˆ›å»ºã€æˆ–è€…é›†ç¾¤èµ„æºå‘ç”Ÿå˜åŒ–æ—¶ï¼Œæ¯”å¦‚ Node èµ„æºä¿¡æ¯å‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦å°†åŸæ¥ unschedulableQ é˜Ÿåˆ—ä¸­è°ƒåº¦å¤±è´¥çš„ Pod è¿›è¡Œé‡æ–°è°ƒåº¦ï¼Œæ­¤å¤„é‡æ–°è°ƒåº¦ä¸»è¦é€šè¿‡å°† unschedulableQ é˜Ÿåˆ—ä¸­ Pod æ·»åŠ åˆ° activeQ æˆ–è€… podBackoffQ é˜Ÿåˆ—ä¸­ï¼Œé€šè¿‡è°ƒç”¨`MoveAllToActiveOrBackoffQueue(event string)` æ–¹æ³•å®ç°ã€‚

```go
// MoveAllToActiveOrBackoffQueue moves all pods from unschedulableQ to activeQ or backoffQ.
// This function adds all pods and then signals the condition variable to ensure that
// if Pop() is waiting for an item, it receives it after all the pods are in the
// queue and the head is the highest priority pod.
func (p *PriorityQueue) MoveAllToActiveOrBackoffQueue(event string) {
   p.lock.Lock()
   defer p.lock.Unlock()
  // åˆ›å»ºä¸ unschedulableQ ä¸­ Pod æ•°é‡ç›¸ç­‰çš„åˆ‡ç‰‡
   unschedulablePods := make([]*framework.QueuedPodInfo, 0, len(p.unschedulableQ.podInfoMap))
   for _, pInfo := range p.unschedulableQ.podInfoMap {
     // å°† unschedulableQ é˜Ÿåˆ—ä¸­æ‰€æœ‰å®¹å™¨å…¨éƒ¨æ·»åŠ åˆ°ä¸Šé¢åˆ›å»ºçš„åˆ‡ç‰‡ä¸­
      unschedulablePods = append(unschedulablePods, pInfo)
   }
  // å°† unschedulableQ ä¸­ Pod æŒ‰ç…§ä¸åŒç±»å‹æ·»åŠ  activeQ æˆ–è€… podBackoffQ é˜Ÿåˆ—ä¸­
   p.movePodsToActiveOrBackoffQueue(unschedulablePods, event)
}

// NOTE: this function assumes lock has been acquired in caller
func (p *PriorityQueue) movePodsToActiveOrBackoffQueue(podInfoList []*framework.QueuedPodInfo, event string) {
   for _, pInfo := range podInfoList {
      pod := pInfo.Pod
     // åˆ¤æ–­å½“å‰ Pod ä»ç„¶å¤„äº podBackoff é‡å¯é˜¶æ®µï¼Œåˆ™å°†è¯¥èŠ‚ç‚¹æ·»åŠ åˆ° podBackoffQ é˜Ÿåˆ—ä¸­
     // ä¸»è¦é€šè¿‡è·å–å½“å‰ pod é‡è¯•æ¬¡æ•° * podInitialBackoffDurationï¼ˆé»˜è®¤1sï¼‰è·å–ä¸‹æ¬¡é‡è¯•éœ€è¦ç­‰å¾…æ—¶é—´
     // å¦‚æœğŸ‘†çš„ç­‰å¾…æ—¶é—´ > podMaxBackoffDuration(é»˜è®¤10s)ï¼Œåˆ™è¯¥ Pod ä¸‹æ¬¡é‡è¯•éœ€è¦ç­‰å¾…10s
     // è·å–ä¸Šæ¬¡ Pod è°ƒåº¦æ—¶é—´ + ç­‰å¾…æ—¶é—´ï¼Œå¦‚æœå‰é¢çš„æ—¶é—´å¤§äºå½“å‰æ—¶é—´ï¼Œåˆ™è¿”å›trueã€‚
      if p.isPodBackingoff(pInfo) {
        // å°†è¿˜æœªå®Œæˆé‡è¯•çš„ Pod ç»§ç»­æ·»åŠ åˆ° podBackoffQ é˜Ÿé¦–ï¼Œä¼˜å…ˆè¿›è¡Œæ¨é€åˆ° activeQ é˜Ÿåˆ—ä¸­
         if err := p.podBackoffQ.Add(pInfo); err != nil {
            klog.Errorf("Error adding pod %v to the backoff queue: %v", pod.Name, err)
         } else {
            metrics.SchedulerQueueIncomingPods.WithLabelValues("backoff", event).Inc()
            p.unschedulableQ.delete(pod)
         }
      } else {
        // å¦‚æœè¯¥ Pod å·²ç»åˆ°äº†é‡è¯•çš„æ—¶é—´ï¼Œåˆ™ç›´æ¥æ¨é€è‡³ activeQ é˜Ÿåˆ—ä¸­è¿›è¡Œä¸‹ä¸€æ¬¡è°ƒåº¦
         if err := p.activeQ.Add(pInfo); err != nil {
            klog.Errorf("Error adding pod %v to the scheduling queue: %v", pod.Name, err)
         } else {
            metrics.SchedulerQueueIncomingPods.WithLabelValues("active", event).Inc()
            p.unschedulableQ.delete(pod)
         }
      }
   }
   // moveRequestCycleç¼“å­˜schedulingCycle, å½“æœªè°ƒåº¦çš„podé‡æ–°è¢«æ·»åŠ åˆ°activeQueueä¸­
   // ä¼šä¿å­˜schedulingCycleåˆ°moveRequestCycleä¸­
   p.moveRequestCycle = p.schedulingCycle
   p.cond.Broadcast()
}
```

ActiveQåŠ å…¥æ“ä½œå¹²äº†å•¥å‘¢ï¼Ÿ

- ä¼šå°†PodåŠ å…¥åˆ°activeQï¼Œå¹¶ä¸”ä»backoffQå’Œ unschedulableQä¸­ç§»é™¤å½“å‰Pod
- åŒæ—¶é€šè¿‡`sync.cond`å¹¿æ’­é€šçŸ¥é˜»å¡åœ¨Popæ“ä½œçš„schedulerè·å–æ–°çš„Pod

```go
// Add adds a pod to the active queue. It should be called only when a new pod
// is added so there is no chance the pod is already in active/unschedulable/backoff queues
func (p *PriorityQueue) Add(pod *v1.Pod) error {
   p.lock.Lock()
   defer p.lock.Unlock()
  // æ–°å»º QueuedPodInfo å¯¹è±¡
   pInfo := p.newQueuedPodInfo(pod)
  // å°†ä¸Šè¿° Pod æ·»åŠ åˆ° activeQ
  // å¦‚æœ activeQ ä¸­å·²ç»å­˜åœ¨è¯¥ Pod åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™ç›´æ¥æ·»åŠ 
   if err := p.activeQ.Add(pInfo); err != nil {
      klog.Errorf("Error adding pod %v to the scheduling queue: %v", nsNameForPod(pod), err)
      return err
   }
  // ä» unschedulableQ ä¸­åˆ é™¤è¯¥ Pod ä¿¡æ¯ï¼Œé˜²æ­¢äºŒæ¬¡è°ƒåº¦
   if p.unschedulableQ.get(pod) != nil {
      klog.Errorf("Error: pod %v is already in the unschedulable queue.", nsNameForPod(pod))
      p.unschedulableQ.delete(pod)
   }
  // ä» podBackoffQ ä¸­åˆ é™¤è¯¥ Pod ä¿¡æ¯ï¼Œé˜²æ­¢äºŒæ¬¡è°ƒåº¦
   // Delete pod from backoffQ if it is backing off
   if err := p.podBackoffQ.Delete(pInfo); err == nil {
      klog.Errorf("Error: pod %v is already in the podBackoff queue.", nsNameForPod(pod))
   }
   metrics.SchedulerQueueIncomingPods.WithLabelValues("active", PodAdd).Inc()
  // å­˜å‚¨ Pod å’Œè¢«æå Nodeï¼Œæ­¤å¤„åˆšåˆšå¼€å§‹è°ƒåº¦ï¼Œæ‰€ä»¥ Node åç§°æ˜¯ç©º
   p.PodNominator.AddNominatedPod(pod, "")
  // å¹¿æ’­é€šçŸ¥æ‰€æœ‰æºç¨‹ï¼Œæœ‰æ–° pod æ·»åŠ ï¼Œå‡†å¤‡å¯¹è¯¥ Pod è¿›è¡Œé¢„é€‰å’Œä¼˜é€‰è¿‡æ»¤
   p.cond.Broadcast()

   return nil
}
```

å¦‚æœè°ƒåº¦å¤±è´¥ä¹‹åï¼Œéœ€è¦å¯¹è°ƒåº¦å¤±è´¥ Pod è¿›è¡Œåˆ†æµåˆ°å…¶ä»–ä¸¤ä¸ªé˜Ÿåˆ—ä¸­ï¼Œä½†æ˜¯åº”è¯¥æ”¾åˆ° unschedulableQ è¿˜æ˜¯ podBackoffQï¼Ÿ

```go
// AddUnschedulableIfNotPresent inserts a pod that cannot be scheduled into
// the queue, unless it is already in the queue. Normally, PriorityQueue puts
// unschedulable pods in `unschedulableQ`. But if there has been a recent move
// request, then the pod is put in `podBackoffQ`.
func (p *PriorityQueue) AddUnschedulableIfNotPresent(pInfo *framework.QueuedPodInfo, podSchedulingCycle int64) error {
	p.lock.Lock()
	defer p.lock.Unlock()
	pod := pInfo.Pod
  // å¦‚æœ unschedulableQ å·²ç»å­˜åœ¨è¯¥ Podï¼Œåˆ™è¿”å›
	if p.unschedulableQ.get(pod) != nil {
		return fmt.Errorf("pod: %v is already present in unschedulable queue", nsNameForPod(pod))
	}

	// Refresh the timestamp since the pod is re-added.
	pInfo.Timestamp = p.clock.Now()
  // å¦‚æœ activeQ å·²ç»å­˜åœ¨è¯¥ Podï¼Œåˆ™è¿”å›
	if _, exists, _ := p.activeQ.Get(pInfo); exists {
		return fmt.Errorf("pod: %v is already present in the active queue", nsNameForPod(pod))
	}
  // // å¦‚æœ podBackoffQ å·²ç»å­˜åœ¨è¯¥ Podï¼Œåˆ™è¿”å›
	if _, exists, _ := p.podBackoffQ.Get(pInfo); exists {
		return fmt.Errorf("pod %v is already present in the backoff queue", nsNameForPod(pod))
	}

	// If a move request has been received, move it to the BackoffQ, otherwise move
	// it to unschedulableQ.
  // å¦‚æœå½“å‰é›†ç¾¤èŠ‚ç‚¹èµ„æºå‘ç”Ÿå˜åŒ–ç­‰æƒ…å†µå‡ºç°ï¼Œåˆ™å°† Pod ç§»åŠ¨åˆ° podBackoffQ
	if p.moveRequestCycle >= podSchedulingCycle {
		if err := p.podBackoffQ.Add(pInfo); err != nil {
			return fmt.Errorf("error adding pod %v to the backoff queue: %v", pod.Name, err)
		}
		metrics.SchedulerQueueIncomingPods.WithLabelValues("backoff", ScheduleAttemptFailure).Inc()
	} else {
    // å¦åˆ™ç§»åŠ¨åˆ° unschedulableQ
		p.unschedulableQ.addOrUpdate(pInfo)
		metrics.SchedulerQueueIncomingPods.WithLabelValues("unschedulable", ScheduleAttemptFailure).Inc()
	}

  // é‡ç½® NominatedPod ä¿¡æ¯
	p.PodNominator.AddNominatedPod(pod, "")
	return nil
}
```

ä¸€èˆ¬æ¥è¯´ï¼Œå½“ä¸€ä¸ªPodä¸èƒ½å¤Ÿè¢«è°ƒåº¦çš„æ—¶å€™ï¼Œå®ƒä¼šè¢«æ”¾åˆ° unschedulableQ ä¸­ï¼Œä½†æ˜¯å¦‚æœæ”¶åˆ°äº†ä¸€ä¸ª`Move Request`ï¼Œé‚£ä¹ˆå°±å°†è¿™ä¸ªPodç§»åˆ°BackoffQã€‚è¿™æ˜¯å› ä¸ºæœ€è¿‘é›†ç¾¤èµ„æºå‘ç”Ÿäº†å˜æ›´ï¼Œå¦‚æœæ”¾åˆ° podBackoffQï¼Œä¼šæ›´å¿«çš„è¿›è¡Œå°è¯•è¿™ä¸ªPodï¼Œæ›´å¿«åœ°ä½¿å®ƒå¾—åˆ°è°ƒåº¦ï¼Œä¸»è¦æ˜¯å› ä¸º podBackoffQ ä¼šæ›´å¿«è¢«æ›´æ–°åˆ° activeQ è¿›è¡Œè°ƒåº¦ã€‚

### PodBackoffQ

podBackoffQä¸»è¦å­˜å‚¨é‚£äº›åœ¨å¤šä¸ªschedulingCycleä¸­ä¾æ—§è°ƒåº¦å¤±è´¥çš„æƒ…å†µä¸‹ï¼Œåˆ™ä¼šé€šè¿‡ä¹‹å‰è¯´çš„backOffæœºåˆ¶ï¼Œå»¶è¿Ÿç­‰å¾…è°ƒåº¦çš„æ—¶é—´ã€‚åŒæ—¶ä¹Ÿæ˜¯ä¸€ä¸ªå †ï¼Œæ¯æ¬¡è·å–å †é¡¶çš„å…ƒç´ ï¼ŒæŸ¥çœ‹æ˜¯å¦åˆ°æœŸï¼Œå¦‚æœåˆ°æœŸåˆ™å°†å…¶Popå‡ºæ¥ï¼ŒåŠ å…¥åˆ°activeQä¸­ã€‚

```go
// flushBackoffQCompleted Moves all pods from backoffQ which have completed backoff in to activeQ
func (p *PriorityQueue) flushBackoffQCompleted() {
   p.lock.Lock()
   defer p.lock.Unlock()
   for {
     // è·å–å½“å‰ podBackoffQ å †é¡¶çš„ Pod
      rawPodInfo := p.podBackoffQ.Peek()
      if rawPodInfo == nil {
         return
      }
      pod := rawPodInfo.(*framework.QueuedPodInfo).Pod
     // è·å–å½“å‰ Pod çš„åˆ°æœŸæ—¶é—´ï¼Œå¦‚æœæœªåˆ°æœŸåˆ™è¿”å›
      boTime := p.getBackoffTime(rawPodInfo.(*framework.QueuedPodInfo))
      if boTime.After(p.clock.Now()) {
         return
      }
     // å¦‚æœæ—¶é—´åˆ°æœŸåˆ™ä» podBackoffQ å¼¹å‡ºè¯¥ Pod
      _, err := p.podBackoffQ.Pop()
      if err != nil {
         klog.Errorf("Unable to pop pod %v from backoff queue despite backoff completion.", nsNameForPod(pod))
         return
      }
     // å°†è¯¥ Pod æ·»åŠ åˆ° activeQ 
      p.activeQ.Add(rawPodInfo)
      metrics.SchedulerQueueIncomingPods.WithLabelValues("active", BackoffComplete).Inc()
     // é€šçŸ¥å„åç¨‹æœ‰æ–° Pod åŠ å…¥
      defer p.cond.Broadcast()
   }
}
```

###UnschedulableQ

unschedulableQ å­˜å‚¨å·²ç»å°è¯•è°ƒåº¦ä½†æ˜¯å½“å‰é›†ç¾¤èµ„æºä¸æ»¡è¶³çš„podçš„é˜Ÿåˆ—ï¼Œå¦‚æœå½“å‰æ—¶é—´-podçš„æœ€åè°ƒåº¦æ—¶é—´å¤§äº60sï¼Œå°±é‡æ–°è°ƒåº¦ï¼Œè½¬ç§»åˆ°podBackoffQæˆ–è€…activeQä¸­ã€‚

```go
// flushUnschedulableQLeftover moves pod which stays in unschedulableQ longer than the unschedulableQTimeInterval
// to activeQ.
func (p *PriorityQueue) flushUnschedulableQLeftover() {
   p.lock.Lock()
   defer p.lock.Unlock()

   var podsToMove []*framework.QueuedPodInfo
   currentTime := p.clock.Now()
   for _, pInfo := range p.unschedulableQ.podInfoMap {
      lastScheduleTime := pInfo.Timestamp
     // å¦‚æœè¯¥ pod 1åˆ†é’Ÿå†…æ²¡æœ‰è¢«è°ƒåº¦å°±åŠ å…¥åˆ° podsToMove
      if currentTime.Sub(lastScheduleTime) > unschedulableQTimeInterval {
         podsToMove = append(podsToMove, pInfo)
      }
   }

   if len(podsToMove) > 0 {
     // podsToMoveå°†è¿™äº›podç§»åŠ¨åˆ°activeQ
      p.movePodsToActiveOrBackoffQueue(podsToMove, UnschedulableTimeout)
   }
}
```
## SchedulerCache æ ¸å¿ƒæºç å®ç°

### schedulerCache 

ä¸ºä»€ä¹ˆéœ€è¦ Scheduler Cache ? è¿™é‡Œçš„Cacheä¸»è¦ç”¨æ¥æ”¶é›†Podå’ŒNodeçº§åˆ«çš„ä¿¡æ¯ï¼Œä¾¿äºGeneric Scheduleråœ¨è°ƒåº¦æ—¶é«˜æ•ˆçš„æŸ¥è¯¢ã€‚

```go
type schedulerCache struct {
	stop   <-chan struct{}
	ttl    time.Duration
	period time.Duration

	// This mutex guards all fields within this cache struct.
  // è¯»å†™é”ç¡®ä¿è¯»å¤šå†™å°‘åœºæ™¯ä¸‹æ•°æ®çš„å®‰å…¨
	mu sync.RWMutex
  
	// a set of assumed pod keys.
	// The key could further be used to get an entry in podStates.
  // ä¸»è¦ç”¨æ¥å­˜å‚¨å·²ç»è¢«è°ƒåº¦å™¨åˆ†é…èŠ‚ç‚¹çš„pod
	assumedPods map[string]bool
  
	// a map from pod key to podState.
  // å­˜å‚¨podå¯¹åº”çš„çŠ¶æ€ï¼ŒçŠ¶æ€ä¸»è¦åŒ…æ‹¬ pod *v1.Podã€deadline *time.Timeã€bindingFinished bool
  // pod ï¼Œå½“å‰ pod å¯¹åº”çš„é…ç½®ä¿¡æ¯
  // deadline ï¼Œç”¨äºè®°å½• pod è¿‡æœŸæ—¶é—´ï¼Œè¶…è¿‡æ”¹æ—¶é—´ç‚¹è®²ç”±cleanupAssumedPodsåˆ é™¤
  // bindingFinishedï¼Œæ ‡è®°å½“å‰podæ˜¯å¦è¢«ç»‘å®šä¸­ï¼Œå¦‚æœfalseåˆ™è¯´æ˜è¿˜æ²¡å®Œæˆç»‘å®šèŠ‚ç‚¹
	podStates map[string]*podState
  
  // ä½¿ç”¨åŒå‘é“¾è¡¨çš„å½¢å¼å­˜å‚¨èŠ‚ç‚¹çš„æ˜ å°„å…³ç³»
  // å¦‚æœæœ‰èŠ‚ç‚¹ä¿¡æ¯å‘ç”Ÿæ›´æ–°ï¼Œå°†ä¼šå°†èŠ‚ç‚¹ä¿¡æ¯æ”¾åˆ°é“¾è¡¨è¡¨å¤´
  // è¶Šé å‰çš„èŠ‚ç‚¹è¶Šæ˜¯æœ€æ–°æ›´æ–°çš„èŠ‚ç‚¹
	nodes     map[string]*nodeInfoListItem
  
	// headNode points to the most recently updated NodeInfo in "nodes". It is the
	// head of the linked list.
  // headNode æŒ‡å‘å½“å‰ğŸ‘† nodes ä¸­æœ€æ–°æ›´æ–°çš„èŠ‚ç‚¹ä¿¡æ¯ï¼Œå³åŒå‘é“¾è¡¨ä¸­æœ€å‰é¢çš„èŠ‚ç‚¹
	headNode *nodeInfoListItem
  
  // nodeTreeæ˜¯ä¸€ä¸ªæ ‘çŠ¶æ•°æ®ç»“æ„ï¼Œåœ¨æ¯ä¸ªåŒºåŸŸä¸­ä¿å­˜èŠ‚ç‚¹åç§°ã€‚ 
  //ç›®çš„æ˜¯ç”¨äºèŠ‚ç‚¹æ‰“æ•£ã€‚èŠ‚ç‚¹æ‰“æ•£ä¸»è¦æ˜¯æŒ‡çš„è°ƒåº¦å™¨è°ƒåº¦çš„æ—¶å€™ï¼Œåœ¨æ»¡è¶³è°ƒåº¦éœ€æ±‚çš„æƒ…å†µä¸‹ï¼Œä¸ºäº†ä¿è¯podå‡åŒ€åˆ†é…åˆ°æ‰€æœ‰çš„nodeèŠ‚ç‚¹ä¸Šï¼Œé€šå¸¸ä¼šæŒ‰ç…§é€ä¸ªzoneé€ä¸ªnodeèŠ‚ç‚¹è¿›è¡Œåˆ†é…ï¼Œä»è€Œè®©podèŠ‚ç‚¹æ‰“æ•£åœ¨æ•´ä¸ªé›†ç¾¤ä¸­ã€‚
	nodeTree *nodeTree
  
	// A map from image name to its imageState.
  // ç”¨äºå­˜å‚¨é•œåƒä¿¡æ¯ï¼ŒåŒ…æ‹¬é•œåƒå¤§å°ã€å­˜åœ¨è¯¥é•œåƒçš„èŠ‚ç‚¹åç§°
	imageStates map[string]*imageState
}
```

###PodçŠ¶æ€

Cacheçš„æ“ä½œéƒ½æ˜¯ä»¥Podä¸ºä¸­å¿ƒçš„ï¼Œå¯¹äºæ¯æ¬¡Pod Eventsï¼ŒCacheä¼šåšé€’å¢å¼updateï¼Œä¸‹é¢æ˜¯Cacheçš„çŠ¶æ€æœºã€‚

```go
// State Machine of a pod's events in scheduler's cache
//   +-------------------------------------------+  +----+
//   |                            Add            |  |    |
//   |                                           |  |    | Update
//   +      Assume                Add            v  v    |
//Initial +--------> Assumed +------------+---> Added <--+
//   ^                +   +               |       +
//   |                |   |               |       |
//   |                |   |           Add |       | Remove
//   |                |   |               |       |
//   |                |   |               +       |
//   +----------------+   +-----------> Expired   +----> Deleted
//         Forget             Expire
```

Pod äº‹ä»¶ï¼š

- Assumeï¼šassumes a pod scheduled and aggregates the podâ€™s information into its node
- Forgetï¼šremoves an assumed pod from cache
- Expireï¼šAfter expiration, its information would be subtracted
- Addï¼šeither confirms a pod if itâ€™s assumed, or adds it back if itâ€™s expired
- Updateï¼šremoves oldPodâ€™s information and adds newPodâ€™s information
- Removeï¼šremoves a pod. The podâ€™s information would be subtracted from assigned node.

ä¸æ­¤åŒæ—¶è¿˜å¯¹åº”æœ‰Podçš„å‡ ç§çŠ¶æ€ï¼Œå…¶ä¸­ Initialã€Expiredã€Deletedè¿™ä¸‰ç§çŠ¶æ€çš„Podåœ¨Cacheä¸­å®é™…ä¸Šæ˜¯ä¸å­˜åœ¨çš„ï¼Œè¿™é‡Œåªæ˜¯ä¸ºäº†çŠ¶æ€æœºçš„è¡¨ç¤ºæ–¹ä¾¿ã€‚
å…³äºè¿™å‡ ä¸ªçŠ¶æ€çš„æ”¹å˜ï¼Œæœ‰ä¸€ä¸ªå…·ä½“çš„å®ç°ç»“æ„ä½“ï¼Œä¸»è¦æ˜¯é€šè¿‡ podState å’Œ assumedPods è¿™ä¸¤ä¸ªmapçš„çŠ¶æ€æ¥å®ç°çš„ã€‚

![kube-scheduler-SchedulerCache](../image/kube-scheduler-SchedulerCache.jpg)

Pod çŠ¶æ€:

- Initialï¼šè¯¥çŠ¶æ€ä¸ºè™šæ‹ŸçŠ¶æ€ï¼Œé»˜è®¤æƒ…å†µä¸‹å°†æ²¡æœ‰è°ƒåº¦çš„ Pod éƒ½ä»»åŠ¡æ˜¯å¤„äºè¯¥çŠ¶æ€
- Assumedï¼šå½“ Pod è°ƒåº¦æˆåŠŸä¸”åˆ†é…ç»‘å®šèŠ‚ç‚¹ä¹‹åï¼ŒPod å°†ä¼šè¢«åŠ å…¥åˆ° assumedPod é˜Ÿåˆ—ä¸­ï¼Œè¯¥é˜Ÿåˆ—ä¸­ Pod å°±å¤„äº Assumed çŠ¶æ€
- Addedï¼šå½“æœ‰æ–° Pod Informer äº‹ä»¶è§¦å‘ï¼Œæœ‰ä¸‰ç§æƒ…å†µ Pod ä¼šè¿›å…¥ Added çŠ¶æ€:
  - æ–°å»ºçš„ Pod ä¸å­˜åœ¨äº cache.podStates å’Œ assumedPods ä¸­ã€‚
  - å½“å‰ Pod å¤„äº assumedPods é˜Ÿåˆ—ä¸­ï¼Œä½†æ˜¯å½“å‰ Pod çœŸå®åˆ†é… Node å’Œ assumedPods ä¸­ Pod åˆ†é…çš„ Node ä¸ä¸€è‡´ï¼Œä¼šè¢«ä» assumedPods é˜Ÿåˆ—ä¸­ç§»é™¤ï¼Œé‡æ–°åŠ å…¥åˆ° cache.podStates é˜Ÿåˆ—ã€‚
  - æœ‰å¯èƒ½ä¹‹å‰ Pod å·²ç»è¿‡æœŸä» assumedPods é˜Ÿåˆ—ä¸­ç§»é™¤ï¼Œä¹Ÿä¼šè¢«é‡æ–°åŠ å…¥åˆ° cache.podStates é˜Ÿåˆ—ã€‚

```go
// è¯¥æ–¹æ³•ä¸º podInformer Added äº‹ä»¶çš„ Handlerï¼Œå³å½“æœ‰ Pod è¢«æ–°å»ºçš„æ—¶å€™ï¼Œä¼šæ‰§è¡Œè¯¥æ–¹æ³•
func (cache *schedulerCache) AddPod(pod *v1.Pod) error {
   key, err := framework.GetPodKey(pod)
   if err != nil {
      return err
   }

   cache.mu.Lock()
   defer cache.mu.Unlock()

   currState, ok := cache.podStates[key]
   switch {
     // å¦‚æœè¯¥ Pod åŒæ—¶å­˜åœ¨äº cache.podStates å’Œ cache.assumedPods ä¸¤ä¸ªé˜Ÿåˆ—ä¸­æ—¶
   case ok && cache.assumedPods[key]:
     // åˆ¤æ–­å®é™… Pod å’Œ cache ä¸­ Pod çš„ Spec.NodeName å­—æ®µæ˜¯å¦ä¸€è‡´
      if currState.pod.Spec.NodeName != pod.Spec.NodeName {
         // The pod was added to a different node than it was assumed to.
         klog.Warningf("Pod %v was assumed to be on %v but got added to %v", key, pod.Spec.NodeName, currState.pod.Spec.NodeName)
         // Clean this up.
        // å¦‚æœ Spec.NodeName å­—æ®µä¸ä¸€è‡´ï¼Œ åˆ™éœ€è¦ä» cache.podStates é˜Ÿåˆ—ä¸­åˆ é™¤è¯¥ Pod
         if err = cache.removePod(currState.pod); err != nil {
            klog.Errorf("removing pod error: %v", err)
         }
        // é‡æ–°æ·»åŠ è¯¥ Pod åˆ° cache.podStates é˜Ÿåˆ—ä¸­
         cache.addPod(pod)
      }
     // è¿™é‡Œæˆ‘ä»¬è®¤å®šï¼ŒPod ä¸åº”è¯¥è¢« assume ä¸¤æ¬¡ï¼Œæ‰€ä»¥æ­¤å¤„éœ€è¦å°†è¯¥ Pod ä» assumedPods ä¸­åˆ é™¤ï¼Œè¿›è¡Œé‡æ–°è°ƒåº¦
      delete(cache.assumedPods, key)
      cache.podStates[key].deadline = nil
      cache.podStates[key].pod = pod
   case !ok:
      // Pod was expired. We should add it back.
      cache.addPod(pod)
      ps := &podState{
         pod: pod,
      }
      cache.podStates[key] = ps
   default:
      return fmt.Errorf("pod %v was already in added state", key)
   }
   return nil
}
```

- Expiredï¼šfinishBinding ä¹‹åï¼Œä¼šæŒ‰ç…§ schedulerCache.ttl è®¾ç½® Pod çš„ deadline æ—¶é—´ï¼Œscheduler åœ¨å¯åŠ¨æ—¶ä¼šå¯åŠ¨ä¸€ä¸ª groutie æ¯ cleanAssumedPeriod = 1 * time.Second æ—¶é—´æ‰§è¡Œä¸€æ¬¡ cleanupExpiredAssumedPods ï¼Œä¼šä» assumedPods å’Œ cache.podStates é˜Ÿåˆ—ä¸­æ¸…é™¤è¿‡æœŸçš„ Pod

```go
func (cache *schedulerCache) run() {
  // å¯åŠ¨åç¨‹æ¯1ç§’æ‰§è¡Œæ¸…ç†è¿‡æœŸ Pod æ“ä½œ
   go wait.Until(cache.cleanupExpiredAssumedPods, cache.period, cache.stop)
}

func (cache *schedulerCache) cleanupExpiredAssumedPods() {
   cache.cleanupAssumedPods(time.Now())
}

// cleanupAssumedPods exists for making test deterministic by taking time as input argument.
// It also reports metrics on the cache size for nodes, pods, and assumed pods.
func (cache *schedulerCache) cleanupAssumedPods(now time.Time) {
   cache.mu.Lock()
   defer cache.mu.Unlock()
   defer cache.updateMetrics()

   // The size of assumedPods should be small
   for key := range cache.assumedPods {
     // ä» assumedPods ä¸­è¯»å– Pod
      ps, ok := cache.podStates[key]
      if !ok {
         klog.Fatal("Key found in assumed set but not in podStates. Potentially a logical error.")
      }
     // åˆ¤æ–­æ˜¯å¦å®Œæˆç»‘å®šï¼Œå¦‚æœæ²¡æœ‰å®Œæˆç»‘å®šåˆ™é€€å‡ºæ¸…ç†
      if !ps.bindingFinished {
         klog.V(5).Infof("Couldn't expire cache for pod %v/%v. Binding is still in progress.",
            ps.pod.Namespace, ps.pod.Name)
         continue
      }
     // å¦‚æœ Pod å·²ç»è¿‡æœŸ
      if now.After(*ps.deadline) {
         klog.Warningf("Pod %s/%s expired", ps.pod.Namespace, ps.pod.Name)
        // æ‰§è¡Œè¿‡æœŸæ“ä½œ
         if err := cache.expirePod(key, ps); err != nil {
            klog.Errorf("ExpirePod failed for %s: %v", key, err)
         }
      }
   }
}

func (cache *schedulerCache) expirePod(key string, ps *podState) error {
  // ä» NodeInfo ä¸­åˆ é™¤è¯¥ Pod ï¼Œä¸»è¦æ¶‰åŠæ¸…ç† NodeInfo ä¸­ podWithAffinityã€podWithRequiredAntiAffinityã€Pods ç­‰æ•°æ®ç»“æ„
   if err := cache.removePod(ps.pod); err != nil {
      return err
   }
  // ä» assumedPods ä¸­åˆ é™¤è¯¥ Pod
   delete(cache.assumedPods, key)
  // ä» podStates ä¸­åˆ é™¤è¯¥ Pod
   delete(cache.podStates, key)
   return nil
}
```

åœ¨Cacheçš„è°ƒåº¦è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æœ‰ä»¥ä¸‹å‡ ä¸ªå‡è®¾:

- Podæ˜¯ä¸ä¼šè¢«Assumeä¸¤æ¬¡çš„
- ä¸€ä¸ªPodå¯èƒ½ä¼šç›´æ¥è¢«Addè€Œä¸ç»è¿‡schedulerï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªä¼šçœ‹è§Add Eventè€Œä¸ä¼šçœ‹è§Assume Event
- å¦‚æœä¸€ä¸ªPodæ²¡æœ‰è¢«Addè¿‡ï¼Œé‚£ä¹ˆä»–ä¸ä¼šè¢«Removeæˆ–è€…Update
- `Expired`å’Œ`Deleted`éƒ½æ˜¯æœ‰æ•ˆçš„æœ€ç»ˆçŠ¶æ€ã€‚

### NodeçŠ¶æ€

åœ¨Cacheä¸­ï¼ŒNodeé€šè¿‡åŒå‘é“¾è¡¨çš„å½¢å¼ä¿å­˜ä¿¡æ¯ï¼š

```go
// nodeInfoListItem holds a NodeInfo pointer and acts as an item in a doubly
// linked list. When a NodeInfo is updated, it goes to the head of the list.
// The items closer to the head are the most recently updated items.
type nodeInfoListItem struct {
   info *framework.NodeInfo
   next *nodeInfoListItem
   prev *nodeInfoListItem
}
```

å…¶ä¸­ï¼Œ`NodeInfo`ä¿å­˜çš„ä¿¡æ¯å¦‚ä¸‹æ‰€ç¤ºï¼ŒåŒ…å«äº†å’ŒNodeç›¸å…³çš„ä¸€ç³»åˆ—ä¿¡æ¯ã€‚

```go
// NodeInfo is node level aggregated information.
type NodeInfo struct {
   // Overall node information.
   node *v1.Node

   // Pods running on the node.
   Pods []*PodInfo

   // The subset of pods with affinity.
   PodsWithAffinity []*PodInfo

   // The subset of pods with required anti-affinity.
   PodsWithRequiredAntiAffinity []*PodInfo

   // Ports allocated on the node.
   UsedPorts HostPortInfo

   // Total requested resources of all pods on this node. This includes assumed
   // pods, which scheduler has sent for binding, but may not be scheduled yet.
   Requested *Resource
   // Total requested resources of all pods on this node with a minimum value
   // applied to each container's CPU and memory requests. This does not reflect
   // the actual resource requests for this node, but is used to avoid scheduling
   // many zero-request pods onto one node.
   NonZeroRequested *Resource
   // We store allocatedResources (which is Node.Status.Allocatable.*) explicitly
   // as int64, to avoid conversions and accessing map.
   Allocatable *Resource

   // ImageStates holds the entry of an image if and only if this image is on the node. The entry can be used for
   // checking an image's existence and advanced usage (e.g., image locality scheduling policy) based on the image
   // state information.
   ImageStates map[string]*ImageStateSummary

   // TransientInfo holds the information pertaining to a scheduling cycle. This will be destructed at the end of
   // scheduling cycle.
   // TODO: @ravig. Remove this once we have a clear approach for message passing across predicates and priorities.
   TransientInfo *TransientSchedulerInfo

   // Whenever NodeInfo changes, generation is bumped.
   // This is used to avoid cloning it if the object didn't change.
   Generation int64
}
```

åœ¨ä¸Šé¢çš„ `schedulerCache` ä¸­é€šè¿‡ `nodes` è¿™ä¸ª map å’Œ `headNode`è¿™ä¸ªæŒ‡é’ˆå¯ä»¥å¾ˆå¿«çš„è®¿é—®Nodeç›¸å…³ä¿¡æ¯ã€‚

#### nodeInfo æ›´æ–°

å½“æ”¶åˆ°informeré€šçŸ¥ï¼ŒçŸ¥é“é›†ç¾¤Nodeä¿¡æ¯å‘ç”Ÿæ”¹å˜æ—¶ï¼Œä¼šæ›´æ–°Cacheä¸­çš„Nodeä¿¡æ¯ã€‚

```go
Handler: cache.ResourceEventHandlerFuncs{
				AddFunc:    sched.addPodToCache,
				UpdateFunc: sched.updatePodInCache,
				DeleteFunc: sched.deletePodFromCache,
			},
```

è¿™é‡Œçš„`add`ã€`update`ã€`delete`ä¼šåˆ†åˆ«è°ƒç”¨Cacheçš„ `AddNode`ã€`UpdateNode`å’Œ `RemoveNode`ç­‰å‡½æ•°ã€‚ä»¥ `AddNode`ä¸ºä¾‹ï¼š

```go
func (cache *schedulerCache) AddNode(node *v1.Node) error {
	cache.mu.Lock()
	defer cache.mu.Unlock()

	n, ok := cache.nodes[node.Name]
	if !ok {
		n = newNodeInfoListItem(framework.NewNodeInfo())
		cache.nodes[node.Name] = n
	} else {
		cache.removeNodeImageStates(n.info.Node())
	}
	cache.moveNodeInfoToHead(node.Name)

	cache.nodeTree.addNode(node)
	cache.addNodeImageStates(node, n.info)
	return n.info.SetNode(node)
}
```

- æ ¹æ®éœ€è¦å¯ä»¥åˆ›å»ºæ–°çš„ NodeInfo ç»“æ„ä½“ï¼Œå¹¶ä¸”æ’å…¥åˆ°åŒå‘é“¾è¡¨ä¸­ã€‚
- æ¯æ¬¡æ›´æ–°Cacheä¸­çš„Nodeä¿¡æ¯æ—¶ï¼Œä¼šå°†è¯¥Nodeç§»åŠ¨åˆ°é“¾è¡¨å¤´ã€‚
- åŒæ—¶ä¼šæ›´æ–° `NodeTree` å’Œ `NodeImageStates`ä¸­çš„ä¿¡æ¯ã€‚

####nodeTree

åœ¨Cacheä¸­è¿˜æœ‰ä¸€ä¸ª`NodeTree`çš„æŒ‡é’ˆç”¨ä¸€ä¸ªæ ‘å½¢ç»“æ„ä½“ä¿å­˜Nodeçš„ç›¸å…³ä¿¡æ¯ï¼Œç›®çš„æ˜¯ç”¨äºèŠ‚ç‚¹æ‰“æ•£ã€‚èŠ‚ç‚¹æ‰“æ•£ä¸»è¦æ˜¯æŒ‡çš„è°ƒåº¦å™¨è°ƒåº¦çš„æ—¶å€™ï¼Œåœ¨æ»¡è¶³è°ƒåº¦éœ€æ±‚çš„æƒ…å†µä¸‹ï¼Œä¸ºäº†ä¿è¯podå‡åŒ€åˆ†é…åˆ°æ‰€æœ‰çš„nodeèŠ‚ç‚¹ä¸Šï¼Œé€šå¸¸ä¼šæŒ‰ç…§é€ä¸ªzoneé€ä¸ªnodeèŠ‚ç‚¹è¿›è¡Œåˆ†é…ï¼Œä»è€Œè®©podèŠ‚ç‚¹æ‰“æ•£åœ¨æ•´ä¸ªé›†ç¾¤ä¸­ã€‚

`NodeTree`çš„ç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼ŒNodeTreeçš„treeæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œkeyæ˜¯zoneçš„åå­—ï¼Œvalueæ˜¯ä¸€ä¸ªnodeArrayï¼Œé€šè¿‡è¿™æ ·å¯ä»¥æŠŠä¸åŒzoneçš„Nodeåˆ†éš”å¼€ã€‚nodeArrayè´Ÿè´£å­˜å‚¨ä¸€ä¸ªzoneä¸‹é¢çš„æ‰€æœ‰nodeèŠ‚ç‚¹ï¼Œå¹¶ä¸”é€šè¿‡lastIndexè®°å½•å½“å‰zoneåˆ†é…çš„èŠ‚ç‚¹ç´¢å¼•ã€‚

```go
type nodeTree struct {
	tree      map[string]*nodeArray // a map from zone (region-zone) to an array of nodes in the zone.
	zones     []string              // a list of all the zones in the tree (keys)
	zoneIndex int
	numNodes  int
}

type nodeArray struct {
	nodes     []string
	lastIndex int
}
```

![kube-scheduler-node-tree](../image/kube-scheduler-node-tree.png)
