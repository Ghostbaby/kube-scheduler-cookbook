# kube-schedulerï¼ˆ1.19ï¼‰ æºç ç¬”è®°

## SchedulingQueueä¸‰çº§è°ƒåº¦é˜Ÿåˆ—

![kube-scheduler-SchedulingQueue](../image/kube-scheduler-SchedulingQueue.png)

SchedulingQueue æ˜¯ä¸€ä¸ª Interfaceï¼Œ ä¸»è¦æä¾›ç”¨ä»¥å®ç°å¯¹ Pod çš„å…¥é˜Ÿå‡ºé˜Ÿæ“ä½œï¼Œä¸Šå›¾ä¸­ `PodBackoffMap` å·²ç»è¢«æ›¿æ¢æˆ`QueuedPodInfo`ï¼Œ`QueuedPodInfo`å°†è´¯ç©¿äºæ•´ä¸ªä¸‰ä¸ªè°ƒåº¦é˜Ÿåˆ—ä¸­ã€‚

```go
// QueuedPodInfo is a Pod wrapper with additional information related to
// the pod's status in the scheduling queue, such as the timestamp when
// it's added to the queue.
type QueuedPodInfo struct {
   Pod *v1.Pod
   // The time pod added to the scheduling queue.
  // ä¸Šæ¬¡è¢«è°ƒåº¦çš„æ—¶é—´æˆ³
   Timestamp time.Time
   // Number of schedule attempts before successfully scheduled.
   // It's used to record the # attempts metric.
  // è°ƒåº¦æˆåŠŸä¹‹å‰é‡è¯•æ¬¡æ•°
   Attempts int
   // The time when the pod is added to the queue for the first time. The pod may be added
   // back to the queue multiple times before it's successfully scheduled.
   // It shouldn't be updated once initialized. It's used to record the e2e scheduling
   // latency for a pod.
  // podç¬¬ä¸€æ¬¡è¢«è°ƒåº¦çš„æ—¶é—´ï¼Œåœ¨è¢«åˆå§‹åŒ–ä¹‹åï¼Œè¯¥æ—¶é—´ä¸ä¼šè¢«æ›´æ–°ï¼Œä¸»è¦ç”¨äºè®¡ç®—è°ƒåº¦è€—æ—¶
   InitialAttemptTimestamp time.Time
}
```



```go
// SchedulingQueue is an interface for a queue to store pods waiting to be scheduled.
// The interface follows a pattern similar to cache.FIFO and cache.Heap and
// makes it easy to use those data structures as a SchedulingQueue.
type SchedulingQueue interface {
   framework.PodNominator
   Add(pod *v1.Pod) error
   // AddUnschedulableIfNotPresent adds an unschedulable pod back to scheduling queue.
   // The podSchedulingCycle represents the current scheduling cycle number which can be
   // returned by calling SchedulingCycle().
   AddUnschedulableIfNotPresent(pod *framework.QueuedPodInfo, podSchedulingCycle int64) error
   // SchedulingCycle returns the current number of scheduling cycle which is
   // cached by scheduling queue. Normally, incrementing this number whenever
   // a pod is popped (e.g. called Pop()) is enough.
   SchedulingCycle() int64
   // Pop removes the head of the queue and returns it. It blocks if the
   // queue is empty and waits until a new item is added to the queue.
   Pop() (*framework.QueuedPodInfo, error)
   Update(oldPod, newPod *v1.Pod) error
   Delete(pod *v1.Pod) error
   MoveAllToActiveOrBackoffQueue(event string)
   AssignedPodAdded(pod *v1.Pod)
   AssignedPodUpdated(pod *v1.Pod)
   PendingPods() []*v1.Pod
   // Close closes the SchedulingQueue so that the goroutine which is
   // waiting to pop items can exit gracefully.
   Close()
   // NumUnschedulablePods returns the number of unschedulable pods exist in the SchedulingQueue.
   NumUnschedulablePods() int
   // Run starts the goroutines managing the queue.
   Run()
}
```

å®é™…ä¸»è¦é€šè¿‡ `PriorityQueue` æ¥å®ç°è°ƒåº¦é˜Ÿåˆ—

```go
// PriorityQueue implements a scheduling queue.
// The head of PriorityQueue is the highest priority pending pod. This structure
// has three sub queues. One sub-queue holds pods that are being considered for
// scheduling. This is called activeQ and is a Heap. Another queue holds
// pods that are already tried and are determined to be unschedulable. The latter
// is called unschedulableQ. The third queue holds pods that are moved from
// unschedulable queues and will be moved to active queue when backoff are completed.
type PriorityQueue struct {
   // PodNominator abstracts the operations to maintain nominated Pods.
   framework.PodNominator

   stop  chan struct{}
   clock util.Clock

   // pod initial backoff duration.
   podInitialBackoffDuration time.Duration
   // pod maximum backoff duration.
   podMaxBackoffDuration time.Duration

   lock sync.RWMutex
   cond sync.Cond

   // activeQ is heap structure that scheduler actively looks at to find pods to
   // schedule. Head of heap is the highest priority pod.
   activeQ *heap.Heap
   // podBackoffQ is a heap ordered by backoff expiry. Pods which have completed backoff
   // are popped from this heap before the scheduler looks at activeQ
   podBackoffQ *heap.Heap
   // unschedulableQ holds pods that have been tried and determined unschedulable.
   unschedulableQ *UnschedulablePodsMap
   // schedulingCycle represents sequence number of scheduling cycle and is incremented
   // when a pod is popped.
   schedulingCycle int64
   // moveRequestCycle caches the sequence number of scheduling cycle when we
   // received a move request. Unscheduable pods in and before this scheduling
   // cycle will be put back to activeQueue if we were trying to schedule them
   // when we received move request.
   moveRequestCycle int64

   // closed indicates that the queue is closed.
   // It is mainly used to let Pop() exit its control loop while waiting for an item.
   closed bool
}
```

å…¶ä¸­åŒ…å«å¦‚ä¸‹ä¸‰çº§è°ƒåº¦é˜Ÿåˆ—ï¼š

- activeQï¼Œæ´»åŠ¨é˜Ÿåˆ—ï¼Œä¸»è¦ç”¨ä»¥å­˜å‚¨å½“å‰æ‰€æœ‰æ­£åœ¨ç­‰å¾…è°ƒåº¦çš„Pod
- unschedulableQï¼Œ ä¸å¯è°ƒåº¦é˜Ÿåˆ—ï¼Œå½“Podç”³è¯·çš„èµ„æºåœ¨å½“å‰é›†ç¾¤ä¸­æ— æ³•å¾—åˆ°æ»¡è¶³æ—¶ï¼Œå°†ä¼šè¢«è°ƒåº¦è‡³è¯¥é˜Ÿåˆ—ä¸­ï¼Œå½“é›†ç¾¤èµ„æºå‘ç”Ÿå˜åŒ–æ—¶ï¼Œå†æ¬¡å¯¹è¯¥é˜Ÿåˆ—è¿›è¡Œè°ƒåº¦å°è¯•
- podBackoffQï¼Œå¤±è´¥é˜Ÿåˆ—ï¼Œå½“podè°ƒåº¦å¤±è´¥ä¹‹åå°†ä¼šå¢åŠ åˆ°è¯¥é˜Ÿåˆ—ï¼Œç­‰å¾…åç»­é‡è¯•ï¼Œåå¤è°ƒåº¦å¤±è´¥çš„Podå°†ä¼šæŒ‰æ­¤å¢é•¿ç­‰å¾…æ—¶é—´ï¼Œé™ä½é‡è¯•æ•ˆç‡ã€‚

podBackoffQ å’Œ unschedulableQï¼Œä¼šå®šæ—¶ä»å‰é¢ä¸¤ä¸ªé˜Ÿåˆ—ä¸­æ‹¿å‡ºPodæ”¾åˆ°activeQé˜Ÿåˆ—ã€‚

- æ¯éš”1ç§’æ‰§è¡Œ `flushBackoffQCompleted`ï¼Œå»æ‰¾åˆ°backoffQä¸­ç­‰å¾…åˆ°æœŸçš„Podï¼Œå°†å…¶æ”¾å…¥åˆ°activeQä¸­
- æ¯éš”30ç§’æ‰§è¡Œ `flushUnschedulableQLeftover`ï¼Œå¦‚æœå½“å‰æ—¶é—´-podçš„æœ€åè°ƒåº¦æ—¶é—´å¤§äº60s,å°±é‡æ–°è°ƒåº¦ï¼Œè½¬ç§»åˆ°podBackoffQæˆ–è€…activeQä¸­

### ActiveQ 

#### èµ„æºå‘ç”Ÿå˜åŒ–æ—¶

å½“æœ‰æ–° Pod è¢«åˆ›å»ºã€æˆ–è€…é›†ç¾¤èµ„æºå‘ç”Ÿå˜åŒ–æ—¶ï¼Œæ¯”å¦‚ Node èµ„æºä¿¡æ¯å‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦å°†åŸæ¥ unschedulableQ é˜Ÿåˆ—ä¸­è°ƒåº¦å¤±è´¥çš„ Pod è¿›è¡Œé‡æ–°è°ƒåº¦ï¼Œæ­¤å¤„é‡æ–°è°ƒåº¦ä¸»è¦é€šè¿‡å°† unschedulableQ é˜Ÿåˆ—ä¸­ Pod æ·»åŠ åˆ° activeQ æˆ–è€… podBackoffQ é˜Ÿåˆ—ä¸­ï¼Œé€šè¿‡è°ƒç”¨`MoveAllToActiveOrBackoffQueue(event string)` æ–¹æ³•å®ç°ã€‚

```go
// MoveAllToActiveOrBackoffQueue moves all pods from unschedulableQ to activeQ or backoffQ.
// This function adds all pods and then signals the condition variable to ensure that
// if Pop() is waiting for an item, it receives it after all the pods are in the
// queue and the head is the highest priority pod.
func (p *PriorityQueue) MoveAllToActiveOrBackoffQueue(event string) {
   p.lock.Lock()
   defer p.lock.Unlock()
  // åˆ›å»ºä¸ unschedulableQ ä¸­ Pod æ•°é‡ç›¸ç­‰çš„åˆ‡ç‰‡
   unschedulablePods := make([]*framework.QueuedPodInfo, 0, len(p.unschedulableQ.podInfoMap))
   for _, pInfo := range p.unschedulableQ.podInfoMap {
     // å°† unschedulableQ é˜Ÿåˆ—ä¸­æ‰€æœ‰å®¹å™¨å…¨éƒ¨æ·»åŠ åˆ°ä¸Šé¢åˆ›å»ºçš„åˆ‡ç‰‡ä¸­
      unschedulablePods = append(unschedulablePods, pInfo)
   }
  // å°† unschedulableQ ä¸­ Pod æŒ‰ç…§ä¸åŒç±»å‹æ·»åŠ  activeQ æˆ–è€… podBackoffQ é˜Ÿåˆ—ä¸­
   p.movePodsToActiveOrBackoffQueue(unschedulablePods, event)
}

// NOTE: this function assumes lock has been acquired in caller
func (p *PriorityQueue) movePodsToActiveOrBackoffQueue(podInfoList []*framework.QueuedPodInfo, event string) {
   for _, pInfo := range podInfoList {
      pod := pInfo.Pod
     // åˆ¤æ–­å½“å‰ Pod ä»ç„¶å¤„äº podBackoff é‡å¯é˜¶æ®µï¼Œåˆ™å°†è¯¥èŠ‚ç‚¹æ·»åŠ åˆ° podBackoffQ é˜Ÿåˆ—ä¸­
     // ä¸»è¦é€šè¿‡è·å–å½“å‰ pod é‡è¯•æ¬¡æ•° * podInitialBackoffDurationï¼ˆé»˜è®¤1sï¼‰è·å–ä¸‹æ¬¡é‡è¯•éœ€è¦ç­‰å¾…æ—¶é—´
     // å¦‚æœğŸ‘†çš„ç­‰å¾…æ—¶é—´ > podMaxBackoffDuration(é»˜è®¤10s)ï¼Œåˆ™è¯¥ Pod ä¸‹æ¬¡é‡è¯•éœ€è¦ç­‰å¾…10s
     // è·å–ä¸Šæ¬¡ Pod è°ƒåº¦æ—¶é—´ + ç­‰å¾…æ—¶é—´ï¼Œå¦‚æœå‰é¢çš„æ—¶é—´å¤§äºå½“å‰æ—¶é—´ï¼Œåˆ™è¿”å›trueã€‚
      if p.isPodBackingoff(pInfo) {
        // å°†è¿˜æœªå®Œæˆé‡è¯•çš„ Pod ç»§ç»­æ·»åŠ åˆ° podBackoffQ é˜Ÿé¦–ï¼Œä¼˜å…ˆè¿›è¡Œæ¨é€åˆ° activeQ é˜Ÿåˆ—ä¸­
         if err := p.podBackoffQ.Add(pInfo); err != nil {
            klog.Errorf("Error adding pod %v to the backoff queue: %v", pod.Name, err)
         } else {
            metrics.SchedulerQueueIncomingPods.WithLabelValues("backoff", event).Inc()
            p.unschedulableQ.delete(pod)
         }
      } else {
        // å¦‚æœè¯¥ Pod å·²ç»åˆ°äº†é‡è¯•çš„æ—¶é—´ï¼Œåˆ™ç›´æ¥æ¨é€è‡³ activeQ é˜Ÿåˆ—ä¸­è¿›è¡Œä¸‹ä¸€æ¬¡è°ƒåº¦
         if err := p.activeQ.Add(pInfo); err != nil {
            klog.Errorf("Error adding pod %v to the scheduling queue: %v", pod.Name, err)
         } else {
            metrics.SchedulerQueueIncomingPods.WithLabelValues("active", event).Inc()
            p.unschedulableQ.delete(pod)
         }
      }
   }
   // moveRequestCycleç¼“å­˜schedulingCycle, å½“æœªè°ƒåº¦çš„podé‡æ–°è¢«æ·»åŠ åˆ°activeQueueä¸­
   // ä¼šä¿å­˜schedulingCycleåˆ°moveRequestCycleä¸­
   p.moveRequestCycle = p.schedulingCycle
   p.cond.Broadcast()
}
```

ActiveQåŠ å…¥æ“ä½œå¹²äº†å•¥å‘¢ï¼Ÿ

- ä¼šå°†PodåŠ å…¥åˆ°activeQï¼Œå¹¶ä¸”ä»backoffQå’Œ unschedulableQä¸­ç§»é™¤å½“å‰Pod
- åŒæ—¶é€šè¿‡`sync.cond`å¹¿æ’­é€šçŸ¥é˜»å¡åœ¨Popæ“ä½œçš„schedulerè·å–æ–°çš„Pod

```go
// Add adds a pod to the active queue. It should be called only when a new pod
// is added so there is no chance the pod is already in active/unschedulable/backoff queues
func (p *PriorityQueue) Add(pod *v1.Pod) error {
   p.lock.Lock()
   defer p.lock.Unlock()
  // æ–°å»º QueuedPodInfo å¯¹è±¡
   pInfo := p.newQueuedPodInfo(pod)
  // å°†ä¸Šè¿° Pod æ·»åŠ åˆ° activeQ
  // å¦‚æœ activeQ ä¸­å·²ç»å­˜åœ¨è¯¥ Pod åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™ç›´æ¥æ·»åŠ 
   if err := p.activeQ.Add(pInfo); err != nil {
      klog.Errorf("Error adding pod %v to the scheduling queue: %v", nsNameForPod(pod), err)
      return err
   }
  // ä» unschedulableQ ä¸­åˆ é™¤è¯¥ Pod ä¿¡æ¯ï¼Œé˜²æ­¢äºŒæ¬¡è°ƒåº¦
   if p.unschedulableQ.get(pod) != nil {
      klog.Errorf("Error: pod %v is already in the unschedulable queue.", nsNameForPod(pod))
      p.unschedulableQ.delete(pod)
   }
  // ä» podBackoffQ ä¸­åˆ é™¤è¯¥ Pod ä¿¡æ¯ï¼Œé˜²æ­¢äºŒæ¬¡è°ƒåº¦
   // Delete pod from backoffQ if it is backing off
   if err := p.podBackoffQ.Delete(pInfo); err == nil {
      klog.Errorf("Error: pod %v is already in the podBackoff queue.", nsNameForPod(pod))
   }
   metrics.SchedulerQueueIncomingPods.WithLabelValues("active", PodAdd).Inc()
  // å­˜å‚¨ Pod å’Œè¢«æå Nodeï¼Œæ­¤å¤„åˆšåˆšå¼€å§‹è°ƒåº¦ï¼Œæ‰€ä»¥ Node åç§°æ˜¯ç©º
   p.PodNominator.AddNominatedPod(pod, "")
  // å¹¿æ’­é€šçŸ¥æ‰€æœ‰æºç¨‹ï¼Œæœ‰æ–° pod æ·»åŠ ï¼Œå‡†å¤‡å¯¹è¯¥ Pod è¿›è¡Œé¢„é€‰å’Œä¼˜é€‰è¿‡æ»¤
   p.cond.Broadcast()

   return nil
}
```

å¦‚æœè°ƒåº¦å¤±è´¥ä¹‹åï¼Œéœ€è¦å¯¹è°ƒåº¦å¤±è´¥ Pod è¿›è¡Œåˆ†æµåˆ°å…¶ä»–ä¸¤ä¸ªé˜Ÿåˆ—ä¸­ï¼Œä½†æ˜¯åº”è¯¥æ”¾åˆ° unschedulableQ è¿˜æ˜¯ podBackoffQï¼Ÿ

```go
// AddUnschedulableIfNotPresent inserts a pod that cannot be scheduled into
// the queue, unless it is already in the queue. Normally, PriorityQueue puts
// unschedulable pods in `unschedulableQ`. But if there has been a recent move
// request, then the pod is put in `podBackoffQ`.
func (p *PriorityQueue) AddUnschedulableIfNotPresent(pInfo *framework.QueuedPodInfo, podSchedulingCycle int64) error {
	p.lock.Lock()
	defer p.lock.Unlock()
	pod := pInfo.Pod
  // å¦‚æœ unschedulableQ å·²ç»å­˜åœ¨è¯¥ Podï¼Œåˆ™è¿”å›
	if p.unschedulableQ.get(pod) != nil {
		return fmt.Errorf("pod: %v is already present in unschedulable queue", nsNameForPod(pod))
	}

	// Refresh the timestamp since the pod is re-added.
	pInfo.Timestamp = p.clock.Now()
  // å¦‚æœ activeQ å·²ç»å­˜åœ¨è¯¥ Podï¼Œåˆ™è¿”å›
	if _, exists, _ := p.activeQ.Get(pInfo); exists {
		return fmt.Errorf("pod: %v is already present in the active queue", nsNameForPod(pod))
	}
  // // å¦‚æœ podBackoffQ å·²ç»å­˜åœ¨è¯¥ Podï¼Œåˆ™è¿”å›
	if _, exists, _ := p.podBackoffQ.Get(pInfo); exists {
		return fmt.Errorf("pod %v is already present in the backoff queue", nsNameForPod(pod))
	}

	// If a move request has been received, move it to the BackoffQ, otherwise move
	// it to unschedulableQ.
  // å¦‚æœå½“å‰é›†ç¾¤èŠ‚ç‚¹èµ„æºå‘ç”Ÿå˜åŒ–ç­‰æƒ…å†µå‡ºç°ï¼Œåˆ™å°† Pod ç§»åŠ¨åˆ° podBackoffQ
	if p.moveRequestCycle >= podSchedulingCycle {
		if err := p.podBackoffQ.Add(pInfo); err != nil {
			return fmt.Errorf("error adding pod %v to the backoff queue: %v", pod.Name, err)
		}
		metrics.SchedulerQueueIncomingPods.WithLabelValues("backoff", ScheduleAttemptFailure).Inc()
	} else {
    // å¦åˆ™ç§»åŠ¨åˆ° unschedulableQ
		p.unschedulableQ.addOrUpdate(pInfo)
		metrics.SchedulerQueueIncomingPods.WithLabelValues("unschedulable", ScheduleAttemptFailure).Inc()
	}

  // é‡ç½® NominatedPod ä¿¡æ¯
	p.PodNominator.AddNominatedPod(pod, "")
	return nil
}
```

ä¸€èˆ¬æ¥è¯´ï¼Œå½“ä¸€ä¸ªPodä¸èƒ½å¤Ÿè¢«è°ƒåº¦çš„æ—¶å€™ï¼Œå®ƒä¼šè¢«æ”¾åˆ° unschedulableQ ä¸­ï¼Œä½†æ˜¯å¦‚æœæ”¶åˆ°äº†ä¸€ä¸ª`Move Request`ï¼Œé‚£ä¹ˆå°±å°†è¿™ä¸ªPodç§»åˆ°BackoffQã€‚è¿™æ˜¯å› ä¸ºæœ€è¿‘é›†ç¾¤èµ„æºå‘ç”Ÿäº†å˜æ›´ï¼Œå¦‚æœæ”¾åˆ° podBackoffQï¼Œä¼šæ›´å¿«çš„è¿›è¡Œå°è¯•è¿™ä¸ªPodï¼Œæ›´å¿«åœ°ä½¿å®ƒå¾—åˆ°è°ƒåº¦ï¼Œä¸»è¦æ˜¯å› ä¸º podBackoffQ ä¼šæ›´å¿«è¢«æ›´æ–°åˆ° activeQ è¿›è¡Œè°ƒåº¦ã€‚

### PodBackoffQ

podBackoffQä¸»è¦å­˜å‚¨é‚£äº›åœ¨å¤šä¸ªschedulingCycleä¸­ä¾æ—§è°ƒåº¦å¤±è´¥çš„æƒ…å†µä¸‹ï¼Œåˆ™ä¼šé€šè¿‡ä¹‹å‰è¯´çš„backOffæœºåˆ¶ï¼Œå»¶è¿Ÿç­‰å¾…è°ƒåº¦çš„æ—¶é—´ã€‚åŒæ—¶ä¹Ÿæ˜¯ä¸€ä¸ªå †ï¼Œæ¯æ¬¡è·å–å †é¡¶çš„å…ƒç´ ï¼ŒæŸ¥çœ‹æ˜¯å¦åˆ°æœŸï¼Œå¦‚æœåˆ°æœŸåˆ™å°†å…¶Popå‡ºæ¥ï¼ŒåŠ å…¥åˆ°activeQä¸­ã€‚

```go
// flushBackoffQCompleted Moves all pods from backoffQ which have completed backoff in to activeQ
func (p *PriorityQueue) flushBackoffQCompleted() {
   p.lock.Lock()
   defer p.lock.Unlock()
   for {
     // è·å–å½“å‰ podBackoffQ å †é¡¶çš„ Pod
      rawPodInfo := p.podBackoffQ.Peek()
      if rawPodInfo == nil {
         return
      }
      pod := rawPodInfo.(*framework.QueuedPodInfo).Pod
     // è·å–å½“å‰ Pod çš„åˆ°æœŸæ—¶é—´ï¼Œå¦‚æœæœªåˆ°æœŸåˆ™è¿”å›
      boTime := p.getBackoffTime(rawPodInfo.(*framework.QueuedPodInfo))
      if boTime.After(p.clock.Now()) {
         return
      }
     // å¦‚æœæ—¶é—´åˆ°æœŸåˆ™ä» podBackoffQ å¼¹å‡ºè¯¥ Pod
      _, err := p.podBackoffQ.Pop()
      if err != nil {
         klog.Errorf("Unable to pop pod %v from backoff queue despite backoff completion.", nsNameForPod(pod))
         return
      }
     // å°†è¯¥ Pod æ·»åŠ åˆ° activeQ 
      p.activeQ.Add(rawPodInfo)
      metrics.SchedulerQueueIncomingPods.WithLabelValues("active", BackoffComplete).Inc()
     // é€šçŸ¥å„åç¨‹æœ‰æ–° Pod åŠ å…¥
      defer p.cond.Broadcast()
   }
}
```

###UnschedulableQ

unschedulableQ å­˜å‚¨å·²ç»å°è¯•è°ƒåº¦ä½†æ˜¯å½“å‰é›†ç¾¤èµ„æºä¸æ»¡è¶³çš„podçš„é˜Ÿåˆ—ï¼Œå¦‚æœå½“å‰æ—¶é—´-podçš„æœ€åè°ƒåº¦æ—¶é—´å¤§äº60sï¼Œå°±é‡æ–°è°ƒåº¦ï¼Œè½¬ç§»åˆ°podBackoffQæˆ–è€…activeQä¸­ã€‚

```go
// flushUnschedulableQLeftover moves pod which stays in unschedulableQ longer than the unschedulableQTimeInterval
// to activeQ.
func (p *PriorityQueue) flushUnschedulableQLeftover() {
   p.lock.Lock()
   defer p.lock.Unlock()

   var podsToMove []*framework.QueuedPodInfo
   currentTime := p.clock.Now()
   for _, pInfo := range p.unschedulableQ.podInfoMap {
      lastScheduleTime := pInfo.Timestamp
     // å¦‚æœè¯¥ pod 1åˆ†é’Ÿå†…æ²¡æœ‰è¢«è°ƒåº¦å°±åŠ å…¥åˆ° podsToMove
      if currentTime.Sub(lastScheduleTime) > unschedulableQTimeInterval {
         podsToMove = append(podsToMove, pInfo)
      }
   }

   if len(podsToMove) > 0 {
     // podsToMoveå°†è¿™äº›podç§»åŠ¨åˆ°activeQ
      p.movePodsToActiveOrBackoffQueue(podsToMove, UnschedulableTimeout)
   }
}
```
## SchedulerCache æ ¸å¿ƒæºç å®ç°

### schedulerCache 

ä¸ºä»€ä¹ˆéœ€è¦ Scheduler Cache ? è¿™é‡Œçš„Cacheä¸»è¦ç”¨æ¥æ”¶é›†Podå’ŒNodeçº§åˆ«çš„ä¿¡æ¯ï¼Œä¾¿äºGeneric Scheduleråœ¨è°ƒåº¦æ—¶é«˜æ•ˆçš„æŸ¥è¯¢ã€‚

```go
type schedulerCache struct {
	stop   <-chan struct{}
	ttl    time.Duration
	period time.Duration

	// This mutex guards all fields within this cache struct.
  // è¯»å†™é”ç¡®ä¿è¯»å¤šå†™å°‘åœºæ™¯ä¸‹æ•°æ®çš„å®‰å…¨
	mu sync.RWMutex
  
	// a set of assumed pod keys.
	// The key could further be used to get an entry in podStates.
  // ä¸»è¦ç”¨æ¥å­˜å‚¨å·²ç»è¢«è°ƒåº¦å™¨åˆ†é…èŠ‚ç‚¹çš„pod
	assumedPods map[string]bool
  
	// a map from pod key to podState.
  // å­˜å‚¨podå¯¹åº”çš„çŠ¶æ€ï¼ŒçŠ¶æ€ä¸»è¦åŒ…æ‹¬ pod *v1.Podã€deadline *time.Timeã€bindingFinished bool
  // pod ï¼Œå½“å‰ pod å¯¹åº”çš„é…ç½®ä¿¡æ¯
  // deadline ï¼Œç”¨äºè®°å½• pod è¿‡æœŸæ—¶é—´ï¼Œè¶…è¿‡æ”¹æ—¶é—´ç‚¹è®²ç”±cleanupAssumedPodsåˆ é™¤
  // bindingFinishedï¼Œæ ‡è®°å½“å‰podæ˜¯å¦è¢«ç»‘å®šä¸­ï¼Œå¦‚æœfalseåˆ™è¯´æ˜è¿˜æ²¡å®Œæˆç»‘å®šèŠ‚ç‚¹
	podStates map[string]*podState
  
  // ä½¿ç”¨åŒå‘é“¾è¡¨çš„å½¢å¼å­˜å‚¨èŠ‚ç‚¹çš„æ˜ å°„å…³ç³»
  // å¦‚æœæœ‰èŠ‚ç‚¹ä¿¡æ¯å‘ç”Ÿæ›´æ–°ï¼Œå°†ä¼šå°†èŠ‚ç‚¹ä¿¡æ¯æ”¾åˆ°é“¾è¡¨è¡¨å¤´
  // è¶Šé å‰çš„èŠ‚ç‚¹è¶Šæ˜¯æœ€æ–°æ›´æ–°çš„èŠ‚ç‚¹
	nodes     map[string]*nodeInfoListItem
  
	// headNode points to the most recently updated NodeInfo in "nodes". It is the
	// head of the linked list.
  // headNode æŒ‡å‘å½“å‰ğŸ‘† nodes ä¸­æœ€æ–°æ›´æ–°çš„èŠ‚ç‚¹ä¿¡æ¯ï¼Œå³åŒå‘é“¾è¡¨ä¸­æœ€å‰é¢çš„èŠ‚ç‚¹
	headNode *nodeInfoListItem
  
  // nodeTreeæ˜¯ä¸€ä¸ªæ ‘çŠ¶æ•°æ®ç»“æ„ï¼Œåœ¨æ¯ä¸ªåŒºåŸŸä¸­ä¿å­˜èŠ‚ç‚¹åç§°ã€‚ 
  //ç›®çš„æ˜¯ç”¨äºèŠ‚ç‚¹æ‰“æ•£ã€‚èŠ‚ç‚¹æ‰“æ•£ä¸»è¦æ˜¯æŒ‡çš„è°ƒåº¦å™¨è°ƒåº¦çš„æ—¶å€™ï¼Œåœ¨æ»¡è¶³è°ƒåº¦éœ€æ±‚çš„æƒ…å†µä¸‹ï¼Œä¸ºäº†ä¿è¯podå‡åŒ€åˆ†é…åˆ°æ‰€æœ‰çš„nodeèŠ‚ç‚¹ä¸Šï¼Œé€šå¸¸ä¼šæŒ‰ç…§é€ä¸ªzoneé€ä¸ªnodeèŠ‚ç‚¹è¿›è¡Œåˆ†é…ï¼Œä»è€Œè®©podèŠ‚ç‚¹æ‰“æ•£åœ¨æ•´ä¸ªé›†ç¾¤ä¸­ã€‚
	nodeTree *nodeTree
  
	// A map from image name to its imageState.
  // ç”¨äºå­˜å‚¨é•œåƒä¿¡æ¯ï¼ŒåŒ…æ‹¬é•œåƒå¤§å°ã€å­˜åœ¨è¯¥é•œåƒçš„èŠ‚ç‚¹åç§°
	imageStates map[string]*imageState
}
```

###PodçŠ¶æ€

Cacheçš„æ“ä½œéƒ½æ˜¯ä»¥Podä¸ºä¸­å¿ƒçš„ï¼Œå¯¹äºæ¯æ¬¡Pod Eventsï¼ŒCacheä¼šåšé€’å¢å¼updateï¼Œä¸‹é¢æ˜¯Cacheçš„çŠ¶æ€æœºã€‚

```go
// State Machine of a pod's events in scheduler's cache
//   +-------------------------------------------+  +----+
//   |                            Add            |  |    |
//   |                                           |  |    | Update
//   +      Assume                Add            v  v    |
//Initial +--------> Assumed +------------+---> Added <--+
//   ^                +   +               |       +
//   |                |   |               |       |
//   |                |   |           Add |       | Remove
//   |                |   |               |       |
//   |                |   |               +       |
//   +----------------+   +-----------> Expired   +----> Deleted
//         Forget             Expire
```

è¿™é‡Œæœ‰å‡ ä¸ªEventéœ€è¦è§£é‡Š

- Assumeï¼šassumes a pod scheduled and aggregates the podâ€™s information into its node
- Forgetï¼šremoves an assumed pod from cache
- Expireï¼šAfter expiration, its information would be subtracted
- Addï¼šeither confirms a pod if itâ€™s assumed, or adds it back if itâ€™s expired
- Updateï¼šremoves oldPodâ€™s information and adds newPodâ€™s information
- Removeï¼šremoves a pod. The podâ€™s information would be subtracted from assigned node.


